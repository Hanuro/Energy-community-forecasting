{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy community forecasting - Final Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Group made of Ivan Duvnjak, Enkh-Oyu Nomin, Oleg Lastocichin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "\n",
    "# import useful libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import statsmodels.tsa.stattools\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import statsmodels.graphics.api as smg\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "\n",
    "# Probabilistic forecasts with neural models and quantile loss\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "from neuralforecast.models import LSTM, DilatedRNN, NHITS\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multiple_pkl_files(file_paths):\n",
    "    data_list = []\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            with open(path, 'rb') as file:\n",
    "                data = pd.read_pickle(file) # use pandas to read pickle files \n",
    "                data_list.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {path}: {e}\")\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['lic_meteo', 'lic_meters', 'lic_nwp']\n",
    "datas = read_multiple_pkl_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo = datas[0]\n",
    "df_meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meters = datas[1]\n",
    "df_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nwp = datas[2]\n",
    "df_nwp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(df):\n",
    "    print(df.shape)\n",
    "    print(\"Null values: \", df.isna().sum().sum())\n",
    "    print(\"Duplicated values: \", df.duplicated().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(datas):\n",
    "    print(f\"Dataset {i}: \")\n",
    "    check_data(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in our main dataset (df_meters) we have only few missing values regarding the total amount of data available and no duplicated values. Therefore, we will manage later the missing values accordingly or eventually manage them through the parameter of the models (missing='drop')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meters.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nwp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 4, figsize=(15, 15))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(20):\n",
    "    axs[i].plot(df_meters[i][\"e_pos\"], color='green', alpha=0.5, label='e_pos')\n",
    "    axs[i].plot(df_meters[i][\"e_neg\"], color='red', alpha=0.5, label='e_neg')\n",
    "    axs[i].set_title(f\"House {i+1}\")\n",
    "    axs[i].legend() \n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that not all houses have values for n_neg, most of them don't. Those that have some values, some follow some seasonality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot values of e_pos & e_neg for \"PCC\" and \"battery\"\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(16, 6))\n",
    "\n",
    "axs[0].plot(df_meters[\"PCC\"][\"e_pos\"], color='green', label='e_pos')\n",
    "axs[0].plot(df_meters[\"PCC\"][\"e_neg\"], color='red', label='e_neg')\n",
    "axs[0].set_title('e_neg & e_pos values for PCC') \n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(df_meters[\"battery\"][\"e_pos\"], color='green', label='e_pos')\n",
    "axs[1].plot(df_meters[\"battery\"][\"e_neg\"], color='red', label='e_neg')\n",
    "axs[1].set_title('e_neg & e_pos values for Battery')  \n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm from the previous plots that e_neg & e_pos are not mutually exclusive. Moreover, most households have e_neg = 0 during the whole time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCC**: There appears to be a cyclical pattern in the data, possibly indicating seasonal variations in energy usage or production. For example, the peaks in energy seem to repeat annually. The red bars which represent \"e_neg\" are significantly smaller and less frequent compared to the positive values. This indicates that instances of negative energy (energy injected back to the grid) are much less common.\n",
    "\n",
    "**Battery**: Unlike the PCC, the battery system shows significant fluctuations between positive and negative energy values. The \"e_neg\" values, particularly, are much more prominent here than in the PCC plot. There are several sharp spikes in both \"e_pos\" and \"e_neg\", which could indicate charging and discharging cycles of the battery. These spikes could also reflect responses to demand shifts or operational adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation - Hourly utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create utilization for all houses, battery, pcc and store as columns in dataframe\n",
    "pdf_meters = pd.DataFrame()\n",
    "\n",
    "for ind in df_meters.columns.levels[0].to_list():\n",
    "    df = df_meters[ind]\n",
    "\n",
    "    df = df.resample('H').mean().dropna()\n",
    "    df['e_utilization'+ \"_\" + str(ind)] = df['e_pos'] + df['e_neg'] #define utilization\n",
    "    df = df.drop(['e_pos','e_neg'],axis=1)\n",
    "\n",
    "    pdf_meters = pd.concat([pdf_meters,df],axis=1)\n",
    "\n",
    "pdf_meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(pdf_meters, title='PCC e_utilization')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Lags which have highest correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Auto correlation \n",
    "acf_df = pd.DataFrame()\n",
    "for cl in pdf_meters.columns:\n",
    "    acf = pd.DataFrame()\n",
    "    acf[cl] = statsmodels.tsa.stattools.acf(pdf_meters[cl], missing=\"drop\", nlags=24) # Auto correlation function\n",
    "    acf_df = pd.concat([acf_df, acf], axis=1)\n",
    "acf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(acf_df, title='ACF e_utilization')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5lags = acf_df.apply(lambda x: pd.Series(x.nlargest(5).index), axis=0)\n",
    "top5lags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We confirm from above auto-correlation analysis that lags 1 and 24 are the most important. This confirms common understanding that energy usage is deterministic from last hour's usage and last day's from same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand relationship between houses and PCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation across series\n",
    "names = pdf_meters.columns.values\n",
    "corr_matrix = np.corrcoef(pdf_meters.T)\n",
    "fig = smg.plot_corr(corr_matrix,xnames=names)\n",
    "# Overall there exist moderate correlation between different series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test if series are stationary\n",
    "https://www.statsmodels.org/dev/examples/notebooks/generated/stationarity_detrending_adf_kpss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADF test** - used to determine the presence of unit root in the series, and hence helps in understand if the series is stationary or not\n",
    "\n",
    "**H0**: The series has a unit root.\n",
    "\n",
    "**H1**: The series has no unit root.\n",
    "\n",
    "If the null hypothesis is failed to be rejected, this test may provide evidence that the series is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return me the non-stationary features by using ADF\n",
    "\n",
    "def check_stationarity_adf(dataframe):\n",
    "    non_stationary_columns = []  # List to hold names of non-stationary columns\n",
    "\n",
    "    for column in dataframe.columns:\n",
    "        dftest = adfuller(dataframe[column].dropna(), autolag='AIC')  # Dropping NA values to avoid errors\n",
    "\n",
    "        # Check if the p-value indicates non-stationarity\n",
    "        if dftest[1] > 0.05:  # p-value greater than 0.05 suggests non-stationarity\n",
    "            non_stationary_columns.append(column)\n",
    "\n",
    "    # Print the results after checking all columns\n",
    "    if non_stationary_columns:\n",
    "        print(f'Non-stationary columns: {\", \".join(non_stationary_columns)}')\n",
    "    else:\n",
    "        print(\"All features are stationary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_stationarity_adf(pdf_meters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPSS is another test for checking the stationarity of a time series. The null and alternate hypothesis for the KPSS test are opposite that of the ADF test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KPSS test**\n",
    "\n",
    "**H0**: The process is trend stationary.\n",
    "\n",
    "**H1**: The series has a unit root (series is not stationary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return me the non-stationary features by using KPSS\n",
    "\n",
    "def kpss_test(dataframe, **kw):\n",
    "    non_stationary_columns = []  # List to store non-stationary columns\n",
    "\n",
    "    # Loop through each column in the DataFrame\n",
    "    for column in dataframe.columns:\n",
    "        series = dataframe[column].dropna()  # Drop NA values to avoid errors\n",
    "        if len(series) == 0:\n",
    "            print(f\"Column {column} only contains NaNs\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            statistic, p_value, n_lags, critical_values = kpss(series, **kw)\n",
    "            print(f'KPSS Test Results for {column}:')\n",
    "            print(f'KPSS Statistic: {statistic}')\n",
    "            print(f'p-value: {p_value}')\n",
    "            print(f'num lags: {n_lags}')\n",
    "            print('Critical Values:')\n",
    "            for key, value in critical_values.items():\n",
    "                print(f'    {key} : {value}')\n",
    "            \n",
    "            # Determine if the series is non-stationary based on the p-value\n",
    "            if p_value < 0.05:\n",
    "                non_stationary_columns.append(column)\n",
    "                print(f'Result: The series {column} is not stationary\\n')\n",
    "            else:\n",
    "                print(f'Result: The series {column} is stationary\\n')\n",
    "\n",
    "        except ValueError as e:  # Catch errors related to insufficient data\n",
    "            print(f\"Error testing {column}: {e}\")\n",
    "\n",
    "    # Print all non-stationary columns at the end of the function execution\n",
    "    if non_stationary_columns:\n",
    "        print(f'Non-stationary columns: {\", \".join(non_stationary_columns)}')\n",
    "    else:\n",
    "        print(\"All columns are stationary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_test(pdf_meters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPSS indicates non-stationarity and ADF indicates stationarity - The series is difference stationary. Differencing is to be used to make series stationary. The differenced series is checked for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_stationary_differencing(dataframe):\n",
    "#     stationary_df = pd.DataFrame()\n",
    "#     for column in dataframe.columns:\n",
    "#         stationary_df[column] = dataframe[column] - dataframe[column].shift(1)\n",
    "\n",
    "#     # dropping the first row which will always be NaN due to differencing\n",
    "#     stationary_df = stationary_df.dropna()\n",
    "\n",
    "#     return stationary_df\n",
    "\n",
    "# stationary_meters = make_stationary_differencing(pdf_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # an example of a feature to verify the result\n",
    "# stationary_meters[\"e_utilization_PCC\"].plot(figsize=(12, 8))\n",
    "# plt.title(\"Differentiated e_utilization_PCC\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # an example of a feature to verify the result\n",
    "# stationary_meters[\"e_utilization_0\"].plot(figsize=(12, 8))\n",
    "# plt.title(\"Differentiated e_utilization_0\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_meters[\"e_utilization_PCC\"] = pdf_meters[\"e_utilization_PCC\"] - pdf_meters[\"e_utilization_PCC\"].shift(1)\n",
    "# pdf_meters[\"e_utilization_PCC\"].dropna().plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_stationarity_adf(stationary_meters) # Check again stationarity after differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kpss_test(stationary_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_stationarity_adf(pdf_meters[\"e_utilization_PCC\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kpss_test(pdf_meters[\"e_utilization_PCC\"].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the series is difference stationary and we have tried to difference them in order to check if the models improved. We got little to no improvements in the models' performances. Therefore, we left the models that we have performed initially that manage the differencing within the model as you will see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_mse_all = {}\n",
    "losses_mae_all = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive method - take the last value as forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nv_fit = pd.DataFrame()\n",
    "nv_fit = pdf_meters.shift(1) # Set the prediction as previous values \n",
    "nv_fit.columns = \"Fitted_\" + nv_fit.columns\n",
    "nv_fit = pd.concat([pdf_meters, nv_fit],axis=1)\n",
    "nv_fit = nv_fit.dropna()\n",
    "nv_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(nv_fit, title='Fitted-Actual Naive')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MSE to understand model performance\n",
    "loss_mae_naive = []\n",
    "loss_mse_naive = []\n",
    "\n",
    "for i, cl in enumerate(pdf_meters.columns):\n",
    "    naive_mse = pd.DataFrame()\n",
    "    pred_cl = \"Fitted_\" + str(cl)\n",
    "    mse_value = mean_squared_error(nv_fit[[cl]], nv_fit[[pred_cl]])  # Calculate MSE once and use it\n",
    "    mae_value = mean_absolute_error(nv_fit[[cl]], nv_fit[[pred_cl]])\n",
    "    \n",
    "    print(cl)\n",
    "    print(f\"MSE: {mse_value.round(3)}\")\n",
    "    print(f\"MAE: {mae_value.round(3)}\")\n",
    "    print()\n",
    "\n",
    "    loss_mae_naive.append(mae_value)\n",
    "    loss_mse_naive.append(mse_value)\n",
    "\n",
    "losses_mae_all[\"naive\"] = loss_mae_naive\n",
    "losses_mse_all[\"naive\"] = loss_mse_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential smoothing with Holt-Winters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_fit = pd.DataFrame()\n",
    "\n",
    "for cl in pdf_meters.columns:\n",
    "    res = pd.DataFrame()\n",
    "    model = ExponentialSmoothing(pdf_meters[cl], trend=\"add\", seasonal=\"add\", seasonal_periods=24).fit() # use exponential smoothing with 24 hours\n",
    "    print(model.summary())\n",
    "    res[cl] = pdf_meters[cl]\n",
    "    f_res = \"Fitted_\" + str(cl)\n",
    "\n",
    "    res[f_res] = model.fittedvalues\n",
    "\n",
    "    hw_fit = pd.concat([hw_fit,res],axis=1)\n",
    "    hw_fit = hw_fit.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(hw_fit, title='Fitted-Actual Holt Winters')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mae_hw = []\n",
    "loss_mse_hw = []\n",
    "\n",
    "for i, cl in enumerate(pdf_meters.columns):\n",
    "    pred_cl = \"Fitted_\" + str(cl)\n",
    "    \n",
    "    mse_value = mean_squared_error(hw_fit[[cl]] , hw_fit[[pred_cl]])  # Calculate MSE once and use it\n",
    "    mae_value = mean_absolute_error(hw_fit[[cl]] , hw_fit[[pred_cl]])\n",
    "    \n",
    "    print(cl)\n",
    "    print(f\"MSE: {mse_value.round(3)}\")\n",
    "    print(f\"MAE: {mae_value.round(3)}\")\n",
    "    print()\n",
    "\n",
    "    loss_mae_hw.append(mae_value)\n",
    "    loss_mse_hw.append(mse_value)\n",
    "\n",
    "losses_mae_all[\"hw\"] = loss_mae_hw\n",
    "losses_mse_all[\"hw\"] = loss_mse_hw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AR-1 : Auto-regressive lag 1, 24 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AR 1,24 model\n",
    "ar1_fit = pd.DataFrame()\n",
    "\n",
    "for cl in pdf_meters.columns:\n",
    "    res = pd.DataFrame()\n",
    "    model = AutoReg(pdf_meters[cl], lags = [1,24], missing=\"drop\").fit()\n",
    "    print(model.summary())\n",
    "    res[cl] = pdf_meters[cl]\n",
    "    f_res = \"Fitted_\" + str(cl)\n",
    "\n",
    "    res[f_res] = model.fittedvalues\n",
    "\n",
    "\n",
    "    ar1_fit = pd.concat([ar1_fit,res],axis=1)\n",
    "    ar1_fit = ar1_fit.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(ar1_fit, title='Fitted-Actual AutoRegressive lags 1,24')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use MsE to understand model performance\n",
    "loss_mae_ar1 = []\n",
    "loss_mse_ar1 = []\n",
    "\n",
    "for i, cl in enumerate(pdf_meters.columns):\n",
    "    pred_cl = \"Fitted_\" + str(cl)\n",
    "    \n",
    "    mse_value = mean_squared_error(ar1_fit[[cl]] , ar1_fit[[pred_cl]])  # Calculate MSE once and use it\n",
    "    mae_value = mean_absolute_error(ar1_fit[[cl]] , ar1_fit[[pred_cl]])\n",
    "    \n",
    "    print(cl)\n",
    "    print(f\"MSE: {mse_value.round(3)}\")\n",
    "    print(f\"MAE: {mae_value.round(3)}\")\n",
    "    print()\n",
    "\n",
    "    loss_mae_ar1.append(mae_value)\n",
    "    loss_mse_ar1.append(mse_value)\n",
    "\n",
    "losses_mae_all[\"ar1\"] = loss_mae_ar1\n",
    "losses_mse_all[\"ar1\"] = loss_mse_ar1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model proposal and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit ARIMA model with differencing to make the series stationary in trend and see if the performance improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA model\n",
    "arima_fit = pd.DataFrame()\n",
    "\n",
    "for cl in pdf_meters.columns:\n",
    "    res = pd.DataFrame()\n",
    "    model = ARIMA(pdf_meters[cl], order=(1,1,1), missing=\"drop\").fit()\n",
    "    print(model.summary())\n",
    "    res[cl] = pdf_meters[cl]\n",
    "    f_res = \"Fitted_\" + str(cl)\n",
    "\n",
    "    res[f_res] = model.fittedvalues\n",
    "\n",
    "\n",
    "    arima_fit = pd.concat([arima_fit,res],axis=1)\n",
    "    arima_fit = arima_fit.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(arima_fit, title='Fitted-Actual ARIMA')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use MsE to understand model performance\n",
    "loss_mae_arima_fit = []\n",
    "loss_mse_arima_fit = []\n",
    "\n",
    "for i, cl in enumerate(pdf_meters.columns):\n",
    "    pred_cl = \"Fitted_\" + str(cl)\n",
    "    \n",
    "    mse_value = mean_squared_error(arima_fit[[cl]] , arima_fit[[pred_cl]])  # Calculate MSE once and use it\n",
    "    mae_value = mean_absolute_error(arima_fit[[cl]] , arima_fit[[pred_cl]])\n",
    "    \n",
    "    print(cl)\n",
    "    print(f\"MSE: {mse_value.round(3)}\")\n",
    "    print(f\"MAE: {mae_value.round(3)}\")\n",
    "    print()\n",
    "\n",
    "    loss_mae_arima_fit.append(mae_value)\n",
    "    loss_mse_arima_fit.append(mse_value)\n",
    "\n",
    "losses_mae_all[\"arima_fit\"] = loss_mae_arima_fit\n",
    "losses_mse_all[\"arima_fit\"] = loss_mse_arima_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving average model without differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MA model\n",
    "ma_fit = pd.DataFrame()\n",
    "\n",
    "for cl in pdf_meters.columns:\n",
    "    res = pd.DataFrame()\n",
    "    model = ARIMA(pdf_meters[cl], order=(0,0,1)).fit()\n",
    "    print(model.summary())\n",
    "    res[cl] = pdf_meters[cl]\n",
    "    f_res = \"Fitted_\" + str(cl)\n",
    "\n",
    "    res[f_res] = model.fittedvalues\n",
    "\n",
    "\n",
    "    ma_fit = pd.concat([ma_fit,res],axis=1)\n",
    "    ma_fit = ma_fit.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(ma_fit, title='Fitted-Actual')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use MsE to understand model performance\n",
    "loss_mae_ma_fit = []\n",
    "loss_mse_ma_fit = []\n",
    "\n",
    "for i, cl in enumerate(pdf_meters.columns):\n",
    "    pred_cl = \"Fitted_\" + str(cl)\n",
    "    \n",
    "    mse_value = mean_squared_error(ma_fit[[cl]] , ma_fit[[pred_cl]])  # Calculate MSE once and use it\n",
    "    mae_value = mean_absolute_error(ma_fit[[cl]] , ma_fit[[pred_cl]])\n",
    "    \n",
    "    print(cl)\n",
    "    print(f\"MSE: {mse_value.round(3)}\")\n",
    "    print(f\"MAE: {mae_value.round(3)}\")\n",
    "    print()\n",
    "\n",
    "    loss_mae_ma_fit.append(mae_value)\n",
    "    loss_mse_ma_fit.append(mse_value)\n",
    "\n",
    "losses_mae_all[\"ma_fit\"] = loss_mae_ma_fit\n",
    "losses_mse_all[\"ma_fit\"] = loss_mse_ma_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_mae = pd.DataFrame(pd.DataFrame(losses_mae_all).T)\n",
    "benchmark_mae.columns = pdf_meters.columns\n",
    "\n",
    "benchmark_mse = pd.DataFrame(pd.DataFrame(losses_mse_all).T)\n",
    "benchmark_mse.columns = pdf_meters.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(benchmark_mae, title='MAE benchmark')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(benchmark_mse, title=\"MSE benchmark\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_mse.T.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_mae.T.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots we see that the error is more or less the same across all models when excluding the PCC, that is really high since all its values are also really high. \n",
    "\n",
    "By looking at the mean value of the table we see that AR1 has the lowest values across the models, both mse and mae."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall, AR1 model performs well as compared to above models, still lets experiment different ARIMA model ( AR , MA, ARMA) and test model performance using CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://alkaline-ml.com/pmdarima/auto_examples/model_selection/example_cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_mae_columns = []\n",
    "losses_mse_columns = []\n",
    "\n",
    "for cl in pdf_meters.columns:\n",
    "    print(\"Model for: \" + cl)\n",
    "    train, test = model_selection.train_test_split(pdf_meters[cl], test_size=24)\n",
    "    \n",
    "    arma = pm.ARIMA(order=(1, 0, 1),\n",
    "                    seasonal_order=(0, 1, 1, 24), # choose 24 for daily seasonality, seen in auto regressive plots\n",
    "                    suppress_warnings=True)\n",
    "    \n",
    "\n",
    "    window_size = np.floor(len(train) / 40) # making the window a bit shorter to save time \n",
    "    step = np.floor(len(train) / 40)\n",
    "    cv = model_selection.SlidingWindowForecastCV(window_size=int(window_size), step=int(step), h=24) \n",
    "    splitter_cv = cv.split(train)\n",
    "\n",
    "    losses_mae = []\n",
    "    losses_mse = []\n",
    "\n",
    "    for i in range(5): # 5 folds \n",
    "        tr_idx, te_idx = next(splitter_cv)\n",
    "        x_tr = train[tr_idx]\n",
    "        y_tr = train[te_idx]\n",
    "\n",
    "\n",
    "        arma.fit(x_tr)\n",
    "        pred = arma.predict(len(y_tr))\n",
    "        mse = mean_squared_error(y_tr, pred)\n",
    "        mae = mean_absolute_error(y_tr, pred)\n",
    "        losses_mae.append(mae)\n",
    "        losses_mse.append(mse)\n",
    "        print(\"MSE \", mse)\n",
    "        print(\"MAE \", mae)\n",
    "\n",
    "        plt.plot(x_tr[-128:], c='blue', label=\"Train\")\n",
    "        plt.plot(pred[:24], c=\"green\", label=\"Forecast\")\n",
    "        plt.plot(y_tr[:24], c=\"red\", label=\"Test\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    losses_mae_columns.append(losses_mae)\n",
    "    losses_mse_columns.append(losses_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots indicate that the model is generally capable of making accurate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_errors = pd.DataFrame(np.array(losses_mae_columns).T, columns=pdf_meters.columns)\n",
    "mse_errors = pd.DataFrame(np.array(losses_mse_columns).T, columns=pdf_meters.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(mae_errors, title='MAE')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(mse_errors, title='MSE')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_errors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_errors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model has a stable performance across the different splits most of the houses.  \n",
    "While for the PCC forecast struggles a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic forecast model using LSTM and NHITS models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined a preprocessing function to prepare the data to be used in a NeuralForecast model, the data had to be defined in a predefined preset that included:\n",
    "- ds = the datetime\n",
    "- y = the target values\n",
    "- unique_id = the column name in our case the house or pcc\n",
    "\n",
    "As such we proceeded to transform the data to satisfy this preset.\n",
    "We converted the datetimes to not include the timezone value.\n",
    "Interpolated missing values, there is no option to drop them like previously used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #function to preprocess each column for the specific models\n",
    "def preprocess_column(column_name, column_data, test_size):\n",
    "    df_lstm = pd.DataFrame(column_data)\n",
    "\n",
    "    df_lstm.rename(columns={column_name: 'y'}, inplace=True) # define target\n",
    "\n",
    "    df_lstm.reset_index(inplace=True)\n",
    "    df_lstm['ds'] = pd.to_datetime(df_lstm['index']) # move the datetimes from index to the column ds\n",
    "    df_lstm.drop(columns=['index'], inplace=True)\n",
    "    # transformations to ds to comply to the needed datatype\n",
    "    df_lstm['ds'] = df_lstm['ds'].dt.tz_localize(None)\n",
    "    df_lstm['ds'] = df_lstm['ds'].astype('datetime64[ns]')\n",
    "    # assigning unique id value for the column pre-processed\n",
    "    df_lstm[\"unique_id\"] = column_name    \n",
    "\n",
    "    # interpolate missing values\n",
    "    df_lstm['y'] = df_lstm['y'].interpolate(method='linear', limit_direction='both')\n",
    "    # splitting\n",
    "    df_tr, df_te = model_selection.train_test_split(df_lstm, test_size=test_size)\n",
    "    return df_tr, df_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create our models, this will be used for all the horizons we decided to work on, which are: 24, 12, 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(horizon, levels):\n",
    "    models = [LSTM(input_size=-1, h=horizon,\n",
    "                loss=MQLoss(level=levels), max_steps=1000),\n",
    "            NHITS(input_size=7*horizon, h=horizon,\n",
    "                    n_freq_downsample=[24, 12, 1],\n",
    "                    loss=MQLoss(level=levels), max_steps=250),]\n",
    "    fcst = NeuralForecast(models=models, freq=\"H\")\n",
    "    return fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple function to save computing time and actual time, it loads or save the models previously trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_or_load_model(model_path, fcst, df_tr):\n",
    "    if os.path.exists(model_path):\n",
    "        fcst = fcst.load(model_path)\n",
    "    else:\n",
    "        fcst.fit(df=df_tr)\n",
    "        fcst.save(model_path)\n",
    "    return fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMAE calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmae = lambda x,y: np.mean(np.abs(x-y))/(np.mean(x)+1e-6) # nmae used by the teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print NMAE and MSE given the name of the model, training dataset, forecasts, test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_measures(model, forecasts, df_tr, df_te):\n",
    "  for uid in df_tr['unique_id'].unique():\n",
    "      df_test = df_te.reset_index().drop(\"index\", axis=1)\n",
    "      y_te = df_test.loc[df_test['unique_id'] == uid, 'y']\n",
    "      y_hat = forecasts.loc[forecasts['unique_id'] == uid, f'{model}-median']\n",
    "      print(f'{uid} NMAE:{nmae(y_te.values.ravel(), y_hat.values.ravel()):0.2e}, MSE:{mean_squared_error(y_te.values.ravel(), y_hat.values.ravel()):0.2e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot the results, using the same values as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(model, forecasts, df_tr, df_te):\n",
    "  q_names = ['lo-90','lo-80','hi-90','hi-80']\n",
    "\n",
    "  fig, ax = plt.subplots(3, 1, figsize=(18, 12))\n",
    "\n",
    "  plt.subplots_adjust(hspace=0.25)\n",
    "  for uid, a in zip(df_tr['unique_id'].unique(), ax.ravel()):\n",
    "      df_test = df_te.reset_index().drop(\"index\", axis=1)\n",
    "      y_te = df_test.loc[df_test['unique_id'] == uid, 'y']\n",
    "\n",
    "      y_hat = forecasts.loc[forecasts['unique_id'] == uid, '{}-median'.format(model)]\n",
    "      \n",
    "      y_te.plot(ax=a)\n",
    "      [a.plot(forecasts.loc[forecasts['unique_id'] == uid, '{}-{}'.format(model, q_str)], color='red', alpha=0.2) for q_str in q_names]\n",
    "      y_hat.plot(ax=a)\n",
    "      a.set_title(f'{uid} NMAE:{nmae(y_te.values.ravel(), y_hat.values.ravel()):0.2e}, MSE:{mean_squared_error(y_te.values.ravel(), y_hat.values.ravel()):0.2e}')\n",
    "      a.legend()\n",
    "  plt.suptitle(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first models, that have an horizon of 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_tr = []\n",
    "dfs_te = []\n",
    "for column_name in pdf_meters.columns:\n",
    "    if column_name == 'unique_id':\n",
    "        continue\n",
    "    if column_name in (\"e_utilization_3\", \"e_utilization_6\", \"e_utilization_PCC\"):\n",
    "        df_tr, df_te = preprocess_column(column_name, pdf_meters[column_name], 24)\n",
    "        dfs_tr.append(df_tr)\n",
    "        dfs_te.append(df_te)\n",
    "\n",
    "# Concatenate all preprocessed DataFrames\n",
    "df_tr = pd.concat(dfs_tr)\n",
    "df_te = pd.concat(dfs_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model using the functions from before, in this case it will be for a 24 step model and will be saved as suck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 24 # 24 steps, 1 day prediction\n",
    "levels = [80, 90]\n",
    "base_model_24 = create_model(horizon, levels)\n",
    "\n",
    "model_path_24 = 'neural_forecaster_model_24'\n",
    "fcst_24 = save_or_load_model(model_path_24, base_model_24, df_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model to forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts_24 = fcst_24.predict()\n",
    "forecasts_24 = forecasts_24.reset_index()\n",
    "forecasts_24.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_measures('LSTM', forecasts_24, df_tr, df_te)\n",
    "print()\n",
    "print_measures('NHITS', forecasts_24, df_tr, df_te)\n",
    "\n",
    "plot_results('LSTM', forecasts_24, df_tr, df_te)\n",
    "plot_results('NHITS', forecasts_24, df_tr, df_te)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forecasted median does follow well the true values for every column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the same idea to a different horizon and see how the results change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_tr = []\n",
    "dfs_te = []\n",
    "\n",
    "for column_name in pdf_meters.columns:\n",
    "    if column_name == 'unique_id':\n",
    "        continue\n",
    "    if column_name in (\"e_utilization_3\", \"e_utilization_6\", \"e_utilization_PCC\"):\n",
    "        df_tr, df_te = preprocess_column(column_name, pdf_meters[column_name], 12) # horizon 12\n",
    "        dfs_tr.append(df_tr)\n",
    "        dfs_te.append(df_te)\n",
    "\n",
    "# Concatenate all preprocessed DataFrames\n",
    "df_tr = pd.concat(dfs_tr)\n",
    "df_te = pd.concat(dfs_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 12 # 12 steps, half day prediction\n",
    "levels = [80, 90]\n",
    "base_model_12 = create_model(horizon, levels)\n",
    "\n",
    "# Check if model file exists\n",
    "model_path_12 = 'neural_forecaster_model_12'\n",
    "\n",
    "fcst_12 = save_or_load_model(model_path_12, base_model_12, df_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use fcst for inference\n",
    "forecasts_12 = fcst_12.predict()\n",
    "forecasts_12 = forecasts_12.reset_index()\n",
    "forecasts_12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te.reset_index().drop(\"index\", axis=1)\n",
    "\n",
    "print_measures('LSTM', forecasts_12, df_tr, df_te)\n",
    "print()\n",
    "print_measures('NHITS', forecasts_12, df_tr, df_te)\n",
    "\n",
    "\n",
    "plot_results('LSTM', forecasts_12, df_tr, df_te)\n",
    "plot_results('NHITS', forecasts_12, df_tr, df_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the plot above the prediction of the median looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we will apply the same concepts but with a narrowed horizon of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_tr = []\n",
    "dfs_te = []\n",
    "for column_name in pdf_meters.columns:\n",
    "    if column_name == 'unique_id':\n",
    "        continue\n",
    "    if column_name in (\"e_utilization_3\", \"e_utilization_6\", \"e_utilization_PCC\"):\n",
    "        df_tr, df_te = preprocess_column(column_name, pdf_meters[column_name], 1)\n",
    "        dfs_tr.append(df_tr)\n",
    "        dfs_te.append(df_te)\n",
    "    \n",
    "df_tr = pd.concat(dfs_tr)\n",
    "df_te = pd.concat(dfs_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path_1 = 'neural_forecaster_model_1'\n",
    "horizon = 1 # 1 steps, 1 hour prediction\n",
    "levels = [80, 90]\n",
    "base_model_1 = create_model(horizon, levels)\n",
    "\n",
    "fcst_1 = save_or_load_model(model_path_1, base_model_1, df_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use fcst for inference\n",
    "forecasts_1 = fcst_1.predict()\n",
    "forecasts_1 = forecasts_1.reset_index()\n",
    "forecasts_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_te.reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_measures(\"LSTM\", forecasts_1, df_tr, df_te)\n",
    "print()\n",
    "print_measures(\"NHITS\", forecasts_1, df_tr, df_te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
